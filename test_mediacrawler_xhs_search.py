#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MediaCrawler å°çº¢ä¹¦æœç´¢åŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•ç›®æ ‡ï¼šæœç´¢å°çº¢ä¹¦çƒ­é—¨å†…å®¹å¹¶åˆ†æç»“æœ
"""

import json
import os
import sys
import subprocess
from datetime import datetime
from pathlib import Path
import shutil

# æœç´¢å…³é”®è¯é…ç½®
SEARCH_KEYWORDS = ["AIå·¥å…·", "æ•ˆç‡ç¥å™¨", "å‰¯ä¸šæé’±", "å°çº¢ä¹¦è¿è¥"]

# è¾“å‡ºé…ç½®
OUTPUT_DIR = Path("mediacrawler_search_results")
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTPUT_FILE = OUTPUT_DIR / f"xhs_search_results_{TIMESTAMP}.json"

def setup_environment():
    """é…ç½®ç¯å¢ƒ"""
    print("=" * 60)
    print("MediaCrawler å°çº¢ä¹¦æœç´¢åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    OUTPUT_DIR.mkdir(exist_ok=True)
    print(f"âœ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: {OUTPUT_DIR}")

    # å¤‡ä»½åŸå§‹é…ç½®
    config_file = "mediacrawler/config/base_config.py"
    backup_file = f"mediacrawler/config/base_config.py.backup_{TIMESTAMP}"

    if not os.path.exists(backup_file):
        shutil.copy(config_file, backup_file)
        print(f"âœ“ åŸå§‹é…ç½®å·²å¤‡ä»½: {backup_file}")

    return config_file

def modify_config(config_file):
    """ä¿®æ”¹é…ç½®æ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("ä¿®æ”¹é…ç½®æ–‡ä»¶...")
    print("=" * 60)

    with open(config_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # ä¿®æ”¹å…³é”®è¯
    new_keywords = ",".join(SEARCH_KEYWORDS)
    content = content.replace(
        'KEYWORDS = "AIå·¥å…·,ChatGPT,ç¼–ç¨‹å­¦ä¹ ,Pythonæ•™ç¨‹,å‰¯ä¸šç¼–ç¨‹,AIç»˜ç”»,æœºå™¨å­¦ä¹ ,å‰ç«¯å¼€å‘"',
        f'KEYWORDS = "{new_keywords}"'
    )
    print(f"âœ“ æœç´¢å…³é”®è¯è®¾ç½®ä¸º: {new_keywords}")

    # ä¿®æ”¹çˆ¬å–æ•°é‡ä¸ºæ¯ä¸ªå…³é”®è¯ 20 ä¸ªç¬”è®°
    content = content.replace(
        'CRAWLER_MAX_NOTES_COUNT = 50',
        'CRAWLER_MAX_NOTES_COUNT = 20'
    )
    print("âœ“ æ¯ä¸ªå…³é”®è¯çˆ¬å–æ•°é‡è®¾ç½®ä¸º: 20")

    # ç¡®ä¿ä¿å­˜ä¸º JSON æ ¼å¼
    content = content.replace(
        'SAVE_DATA_OPTION = "json"',
        'SAVE_DATA_OPTION = "json"'
    )

    # è®¾ç½®æ•°æ®ä¿å­˜è·¯å¾„
    content = content.replace(
        'SAVE_DATA_PATH = ""',
        f'SAVE_DATA_PATH = "{OUTPUT_DIR}"'
    )
    print(f"âœ“ æ•°æ®ä¿å­˜è·¯å¾„è®¾ç½®ä¸º: {OUTPUT_DIR}")

    # å…³é—­è¯äº‘ç”Ÿæˆï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰
    content = content.replace(
        'ENABLE_GET_WORDCLOUD = False',
        'ENABLE_GET_WORDCLOUD = False'
    )

    # ä¿å­˜é…ç½®
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write(content)
    print("âœ“ é…ç½®æ–‡ä»¶å·²æ›´æ–°")

def run_crawler():
    """è¿è¡Œ MediaCrawler"""
    print("\n" + "=" * 60)
    print("è¿è¡Œ MediaCrawler çˆ¬è™«...")
    print("=" * 60)

    os.chdir("mediacrawler")

    # ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒè¿è¡Œ
    cmd = [
        ".venv/bin/python",
        "main.py",
        "--platform", "xhs",
        "--lt", "qrcode",
        "--type", "search"
    ]

    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    print("\næ³¨æ„ï¼šè¿™å°†æ‰“å¼€æµè§ˆå™¨ï¼Œè¯·ä½¿ç”¨å°çº¢ä¹¦APPæ‰«ç ç™»å½•...")

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
        print("\n" + "=" * 60)
        print("çˆ¬è™«æ‰§è¡Œå®Œæˆ")
        print("=" * 60)

        if result.stdout:
            print("æ ‡å‡†è¾“å‡º:")
            print(result.stdout[-2000:])  # åªæ˜¾ç¤ºæœ€å2000å­—ç¬¦

        if result.stderr:
            print("é”™è¯¯è¾“å‡º:")
            print(result.stderr[-2000:])

        return result.returncode == 0

    except subprocess.TimeoutExpired:
        print("é”™è¯¯ï¼šçˆ¬è™«æ‰§è¡Œè¶…æ—¶ï¼ˆ10åˆ†é’Ÿï¼‰")
        return False
    except Exception as e:
        print(f"é”™è¯¯ï¼šçˆ¬è™«æ‰§è¡Œå¤±è´¥ - {e}")
        return False
    finally:
        os.chdir("..")

def collect_results():
    """æ”¶é›†æœç´¢ç»“æœ"""
    print("\n" + "=" * 60)
    print("æ”¶é›†æœç´¢ç»“æœ...")
    print("=" * 60)

    results = []

    # æ‰«æè¾“å‡ºç›®å½•ä¸­çš„ JSON æ–‡ä»¶
    json_files = list(OUTPUT_DIR.glob("*.json"))

    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                results.extend(data)
            print(f"âœ“ å·²åŠ è½½: {json_file.name} ({len(data)} æ¡è®°å½•)")
        except Exception as e:
            print(f"âœ— åŠ è½½å¤±è´¥: {json_file.name} - {e}")

    print(f"\næ€»å…±æ”¶é›†åˆ° {len(results)} æ¡ç¬”è®°")

    return results

def analyze_results(results):
    """åˆ†ææœç´¢ç»“æœ"""
    print("\n" + "=" * 60)
    print("åˆ†ææœç´¢ç»“æœ...")
    print("=" * 60)

    if not results:
        print("é”™è¯¯ï¼šæ²¡æœ‰æœç´¢ç»“æœ")
        return None

    analysis = {
        "total_notes": len(results),
        "keywords": SEARCH_KEYWORDS,
        "notes": [],
        "trends": {},
        "content_suggestions": [],
        "best_practices": []
    }

    # ç»Ÿè®¡æ•°æ®
    total_views = 0
    total_likes = 0
    total_collects = 0
    title_lengths = []
    image_counts = []

    for note in results:
        note_data = {
            "title": note.get("title", ""),
            "url": note.get("note_id", ""),
            "views": note.get("view_count", 0),
            "likes": note.get("liked_count", 0),
            "collects": note.get("collected_count", 0),
            "comments": note.get("comment_count", 0),
            "title_length": len(note.get("title", "")),
            "image_count": len(note.get("images", [])),
            "content": note.get("desc", "")[:200]  # å‰200å­—ç¬¦
        }

        analysis["notes"].append(note_data)

        # ç´¯è®¡ç»Ÿè®¡
        total_views += note_data["views"]
        total_likes += note_data["likes"]
        total_collects += note_data["collects"]
        title_lengths.append(note_data["title_length"])
        image_counts.append(note_data["image_count"])

    # è®¡ç®—å¹³å‡å€¼
    if results:
        analysis["trends"]["avg_views"] = total_views // len(results)
        analysis["trends"]["avg_likes"] = total_likes // len(results)
        analysis["trends"]["avg_collects"] = total_collects // len(results)
        analysis["trends"]["avg_title_length"] = sum(title_lengths) // len(title_lengths)
        analysis["trends"]["avg_image_count"] = sum(image_counts) // len(image_counts)

        # æ‰¾å‡ºæœ€å—æ¬¢è¿çš„ç¬”è®°
        sorted_notes = sorted(analysis["notes"], key=lambda x: x["likes"], reverse=True)
        analysis["trends"]["top_notes"] = sorted_notes[:10]

        # æ‰¾å‡ºæ ‡é¢˜æœ€é•¿çš„ç¬”è®°
        sorted_by_title = sorted(analysis["notes"], key=lambda x: x["title_length"], reverse=True)
        analysis["trends"]["longest_titles"] = sorted_by_title[:5]

        # æ‰¾å‡ºé…å›¾æœ€å¤šçš„ç¬”è®°
        sorted_by_images = sorted(analysis["notes"], key=lambda x: x["image_count"], reverse=True)
        analysis["trends"]["most_images"] = sorted_by_images[:5]

    print(f"âœ“ ç¬”è®°æ€»æ•°: {analysis['total_notes']}")
    print(f"âœ“ å¹³å‡æµè§ˆé‡: {analysis['trends'].get('avg_views', 0)}")
    print(f"âœ“ å¹³å‡ç‚¹èµæ•°: {analysis['trends'].get('avg_likes', 0)}")
    print(f"âœ“ å¹³å‡æ”¶è—æ•°: {analysis['trends'].get('avg_collects', 0)}")
    print(f"âœ“ å¹³å‡æ ‡é¢˜é•¿åº¦: {analysis['trends'].get('avg_title_length', 0)} å­—")
    print(f"âœ“ å¹³å‡é…å›¾æ•°: {analysis['trends'].get('avg_image_count', 0)} å¼ ")

    # ç”Ÿæˆå†…å®¹å»ºè®®
    analysis["content_suggestions"] = generate_content_suggestions(analysis)
    analysis["best_practices"] = generate_best_practices(analysis)

    return analysis

def generate_content_suggestions(analysis):
    """ç”Ÿæˆå†…å®¹å»ºè®®"""
    suggestions = []

    trends = analysis.get("trends", {})
    top_notes = trends.get("top_notes", [])

    if top_notes:
        suggestions.append("ğŸ“Œ çƒ­é—¨ç¬”è®°æ ‡é¢˜ç‰¹å¾åˆ†æï¼š")

        for note in top_notes[:5]:
            title = note.get("title", "")
            likes = note.get("likes", 0)
            suggestions.append(f"   - {title[:50]}... (ç‚¹èµ: {likes})")

    suggestions.append("\nğŸ¯ å†…å®¹ä¼˜åŒ–å»ºè®®ï¼š")
    avg_title_len = trends.get("avg_title_length", 0)
    avg_images = trends.get("avg_image_count", 0)

    if avg_title_len > 0:
        suggestions.append(f"   - å»ºè®®æ ‡é¢˜é•¿åº¦åœ¨ {avg_title_len-5}-{avg_title_len+5} å­—ä¹‹é—´")

    if avg_images > 0:
        suggestions.append(f"   - å»ºè®®é…å›¾æ•°é‡åœ¨ {avg_images-1}-{avg_images+2} å¼ å·¦å³")

    suggestions.append("   - æ ‡é¢˜ä¸­åŒ…å«æ•°å­—ã€ç–‘é—®å¥ã€æ„Ÿå¹å¥æ›´å®¹æ˜“å¸å¼•ç‚¹å‡»")
    suggestions.append("   - å°é¢å›¾åº”æ¸…æ™°ã€ç¾è§‚ï¼Œçªå‡ºä¸»é¢˜")
    suggestions.append("   - å†…å®¹è¦æœ‰å®ç”¨æ€§ï¼Œè§£å†³ç”¨æˆ·ç—›ç‚¹")
    suggestions.append("   - åˆç†ä½¿ç”¨æ ‡ç­¾å’Œè¯é¢˜ï¼Œæé«˜æ›å…‰")

    return suggestions

def generate_best_practices(analysis):
    """ç”Ÿæˆæœ€ä½³å®è·µå»ºè®®"""
    practices = []

    practices.append("ğŸ“š å°çº¢ä¹¦çƒ­é—¨ç¬”è®°æœ€ä½³å®è·µï¼š")

    practices.append("\n1ï¸âƒ£ æ ‡é¢˜ä¼˜åŒ–ï¼š")
    practices.append("   - ä½¿ç”¨ç–‘é—®å¥ï¼š'å¦‚ä½•...''ä¸ºä»€ä¹ˆ...'")
    practices.append("   - ä½¿ç”¨æ•°å­—ï¼š'5ä¸ªæŠ€å·§''3ç§æ–¹æ³•'")
    practices.append("   - çªå‡ºç—›ç‚¹ï¼š'è§£å†³...é—®é¢˜''å‘Šåˆ«...çƒ¦æ¼'")
    practices.append("   - æ·»åŠ è¡¨æƒ…ç¬¦å·ï¼Œå¢åŠ è§†è§‰å¸å¼•åŠ›")

    practices.append("\n2ï¸âƒ£ å†…å®¹ç»“æ„ï¼š")
    practices.append("   - å¼€å¤´ç‚¹æ˜ä¸»é¢˜ï¼Œå¿«é€ŸæŠ“ä½ç”¨æˆ·æ³¨æ„")
    practices.append("   - ä¸­é—´è¯¦ç»†å±•å¼€ï¼Œæä¾›å®ç”¨ä»·å€¼")
    practices.append("   - ç»“å°¾å¼•å¯¼äº’åŠ¨ï¼šç‚¹èµã€æ”¶è—ã€è¯„è®º")
    practices.append("   - é€‚å½“åˆ†æ®µï¼Œä½¿ç”¨å°æ ‡é¢˜æé«˜å¯è¯»æ€§")

    practices.append("\n3ï¸âƒ£ é…å›¾ç­–ç•¥ï¼š")
    practices.append("   - å°é¢å›¾ï¼šé«˜æ¸…ã€ç¾è§‚ã€ä¿¡æ¯æ˜ç¡®")
    practices.append("   - å†…å®¹å›¾ï¼šå›¾æ–‡ç»“åˆï¼Œä¿¡æ¯ä¸°å¯Œ")
    practices.append("   - å›¾ç‰‡å°ºå¯¸ï¼šå»ºè®® 1080x1440 æˆ– 3:4 æ¯”ä¾‹")
    practices.append("   - ä½¿ç”¨ç»Ÿä¸€é£æ ¼ï¼Œå»ºç«‹ä¸ªäººå“ç‰Œ")

    practices.append("\n4ï¸âƒ£ å‘å¸ƒæ—¶é—´ï¼š")
    practices.append("   - å·¥ä½œæ—¥ï¼š12:00-13:00, 18:00-22:00")
    practices.append("   - å‘¨æœ«ï¼š09:00-11:00, 15:00-22:00")
    practices.append("   - æ ¹æ®ç›®æ ‡ç”¨æˆ·æ´»è·ƒæ—¶é—´è°ƒæ•´")

    practices.append("\n5ï¸âƒ£ äº’åŠ¨ç­–ç•¥ï¼š")
    practices.append("   - åŠæ—¶å›å¤è¯„è®ºï¼Œå¢åŠ ç”¨æˆ·ç²˜æ€§")
    practices.append("   - åœ¨ç»“å°¾å¼•å¯¼ç”¨æˆ·ï¼š'è§‰å¾—æœ‰ç”¨è¯·æ”¶è—'")
    practices.append("   - å‚ä¸çƒ­é—¨è¯é¢˜ï¼Œå¢åŠ æ›å…‰æœºä¼š")
    practices.append("   - ä¸å…¶ä»–åšä¸»äº’åŠ¨ï¼Œæ‰©å¤§å½±å“åŠ›")

    return practices

def generate_report(analysis):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    print("=" * 60)

    report_lines = []

    report_lines.append("# MediaCrawler å°çº¢ä¹¦æœç´¢åŠŸèƒ½æµ‹è¯•æŠ¥å‘Š")
    report_lines.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"\n## æœç´¢å…³é”®è¯")
    report_lines.append(", ".join(SEARCH_KEYWORDS))

    report_lines.append(f"\n## æœç´¢ç»“æœç»Ÿè®¡")
    report_lines.append(f"- ç¬”è®°æ€»æ•°: {analysis['total_notes']}")
    report_lines.append(f"- å¹³å‡æµè§ˆé‡: {analysis['trends'].get('avg_views', 0)}")
    report_lines.append(f"- å¹³å‡ç‚¹èµæ•°: {analysis['trends'].get('avg_likes', 0)}")
    report_lines.append(f"- å¹³å‡æ”¶è—æ•°: {analysis['trends'].get('avg_collects', 0)}")
    report_lines.append(f"- å¹³å‡æ ‡é¢˜é•¿åº¦: {analysis['trends'].get('avg_title_length', 0)} å­—")
    report_lines.append(f"- å¹³å‡é…å›¾æ•°: {analysis['trends'].get('avg_image_count', 0)} å¼ ")

    report_lines.append("\n## çƒ­é—¨ç¬”è®° TOP 10")
    top_notes = analysis['trends'].get('top_notes', [])
    for i, note in enumerate(top_notes[:10], 1):
        report_lines.append(f"\n### {i}. {note.get('title', '')}")
        report_lines.append(f"- URL: {note.get('url', '')}")
        report_lines.append(f"- æµè§ˆé‡: {note.get('views', 0)}")
        report_lines.append(f"- ç‚¹èµ: {note.get('likes', 0)}")
        report_lines.append(f"- æ”¶è—: {note.get('collects', 0)}")
        report_lines.append(f"- è¯„è®º: {note.get('comments', 0)}")
        report_lines.append(f"- æ ‡é¢˜é•¿åº¦: {note.get('title_length', 0)} å­—")
        report_lines.append(f"- é…å›¾æ•°: {note.get('image_count', 0)} å¼ ")
        report_lines.append(f"- å†…å®¹é¢„è§ˆ: {note.get('content', '')[:100]}...")

    report_lines.append("\n## å†…å®¹è¶‹åŠ¿åˆ†æ")
    report_lines.append("\n### æ ‡é¢˜ç‰¹å¾")
    longest_titles = analysis['trends'].get('longest_titles', [])
    report_lines.append("æ ‡é¢˜è¾ƒé•¿çš„ç¬”è®°ç¤ºä¾‹ï¼š")
    for note in longest_titles[:3]:
        report_lines.append(f"- {note.get('title', '')} ({note.get('title_length', 0)} å­—)")

    report_lines.append("\n### é…å›¾ç­–ç•¥")
    most_images = analysis['trends'].get('most_images', [])
    report_lines.append("é…å›¾è¾ƒå¤šçš„ç¬”è®°ç¤ºä¾‹ï¼š")
    for note in most_images[:3]:
        report_lines.append(f"- {note.get('title', '')} ({note.get('image_count', 0)} å¼ å›¾)")

    report_lines.append("\n## å†…å®¹å»ºè®®")
    for suggestion in analysis['content_suggestions']:
        report_lines.append(f"{suggestion}")

    report_lines.append("\n## æœ€ä½³å®è·µå»ºè®®")
    for practice in analysis['best_practices']:
        report_lines.append(f"{practice}")

    # ä¿å­˜æŠ¥å‘Š
    report_file = OUTPUT_DIR / f"xhs_search_report_{TIMESTAMP}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))

    print(f"âœ“ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    # åŒæ—¶ä¿å­˜ JSON æ ¼å¼çš„åˆ†æç»“æœ
    analysis_file = OUTPUT_DIR / f"xhs_search_analysis_{TIMESTAMP}.json"
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)

    print(f"âœ“ åˆ†ææ•°æ®å·²ä¿å­˜: {analysis_file}")

    return report_file

def main():
    """ä¸»å‡½æ•°"""
    try:
        # 1. è®¾ç½®ç¯å¢ƒ
        config_file = setup_environment()

        # 2. ä¿®æ”¹é…ç½®
        modify_config(config_file)

        # 3. è¿è¡Œçˆ¬è™«
        success = run_crawler()

        if not success:
            print("\né”™è¯¯ï¼šçˆ¬è™«æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            return 1

        # 4. æ”¶é›†ç»“æœ
        results = collect_results()

        if not results:
            print("\nè­¦å‘Šï¼šæ²¡æœ‰æ”¶é›†åˆ°æœç´¢ç»“æœ")
            return 1

        # 5. åˆ†æç»“æœ
        analysis = analyze_results(results)

        if not analysis:
            print("\né”™è¯¯ï¼šç»“æœåˆ†æå¤±è´¥")
            return 1

        # 6. ç”ŸæˆæŠ¥å‘Š
        report_file = generate_report(analysis)

        print("\n" + "=" * 60)
        print("âœ“ æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        print(f"\næŸ¥çœ‹å®Œæ•´æŠ¥å‘Š: {report_file}")

        return 0

    except Exception as e:
        print(f"\né”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
