#!/usr/bin/env python3
"""
AIå‰æ²¿ä¿¡æ¯çˆ¬è™«
é‡‡é›† arXivã€Hacker Newsã€GitHub ç­‰å¹³å°çš„AIç›¸å…³ä¿¡æ¯
"""

import requests
import feedparser
from datetime import datetime, timedelta
from typing import List, Dict
import json
import os

# æ•°æ®è¾“å‡ºè·¯å¾„
OUTPUT_DIR = "/home/vimalinx/.openclaw/workspace/ai_insights/data"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def fetch_arxiv_papers(days_back=7, limit=15) -> List[Dict]:
    """
    ä» arXiv è·å–æœ€æ–°AI/MLè®ºæ–‡

    ä½¿ç”¨ arXiv API: http://export.arxiv.org/api/query
    """
    print("ğŸ“š æ­£åœ¨ä» arXiv è·å–æœ€æ–°è®ºæ–‡...")

    # æœç´¢æœ€è¿‘Nå¤©çš„è®ºæ–‡
    categories = [
        "cs.AI",        # Artificial Intelligence
        "cs.LG",        # Machine Learning
        "cs.CL",        # Computation and Language
        "cs.CV",        # Computer Vision
        "cs.NE",        # Neural and Evolutionary Computing
    ]

    papers = []
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days_back)

    for category in categories:
        query = f"cat:{category}"
        url = f"http://export.arxiv.org/api/query?search_query={query}&start=0&max_results={limit//len(categories)}&sortBy=submittedDate&sortOrder=descending"

        try:
            feed = feedparser.parse(url)

            for entry in feed.entries:
                # è§£æä½œè€…
                authors = [author.name for author in entry.authors]

                # è§£ææ‘˜è¦ï¼ˆå»æ‰å¤šä½™ç©ºæ ¼ï¼‰
                summary = ' '.join(entry.summary.split())

                papers.append({
                    "title": entry.title,
                    "authors": authors[:4] + ["ç­‰"] if len(authors) > 4 else authors,
                    "summary": summary[:500] + "..." if len(summary) > 500 else summary,
                    "link": entry.id.replace("http://arxiv.org/abs/", "https://arxiv.org/abs/"),
                    "category": category,
                    "published": entry.published,
                })
        except Exception as e:
            print(f"  âš ï¸  {category} æŠ“å–å¤±è´¥: {e}")

    return papers[:limit]


def fetch_hacker_news_ai(limit=10) -> List[Dict]:
    """
    ä» Hacker News è·å–AIç›¸å…³çƒ­é—¨è®¨è®º

    ä½¿ç”¨ HN API: https://github.com/HackerNews/API
    """
    print("ğŸ’¬ æ­£åœ¨ä» Hacker News è·å–AIè®¨è®º...")

    base_url = "https://hacker-news.firebaseio.com/v0"
    ai_keywords = ["AI", "ML", "machine learning", "deep learning", "GPT", "LLM", "neural"]

    # è·å–æœ€æ–°æ•…äº‹IDåˆ—è¡¨
    try:
        new_stories = requests.get(f"{base_url}/newstories.json").json()
        top_stories = requests.get(f"{base_url}/topstories.json").json()
        story_ids = list(dict.fromkeys(new_stories[:200] + top_stories[:200]))  # å»é‡åˆå¹¶
    except Exception as e:
        print(f"  âš ï¸  è·å–æ•…äº‹åˆ—è¡¨å¤±è´¥: {e}")
        return []

    discussions = []
    checked_ids = set()

    for story_id in story_ids:
        if len(discussions) >= limit:
            break

        if story_id in checked_ids:
            continue
        checked_ids.add(story_id)

        try:
            story = requests.get(f"{base_url}/item/{story_id}.json").json()

            # æ£€æŸ¥æ˜¯å¦ä¸AIç›¸å…³
            title = story.get("title", "")
            text = story.get("text", "")

            is_ai_related = any(
                keyword.lower() in title.lower() or keyword.lower() in text.lower()
                for keyword in ai_keywords
            )

            if not is_ai_related:
                continue

            discussions.append({
                "title": title,
                "url": story.get("url", f"https://news.ycombinator.com/item?id={story_id}"),
                "score": story.get("score", 0),
                "comments": story.get("descendants", 0),
                "time": datetime.fromtimestamp(story.get("time", 0)).strftime("%Y-%m-%d"),
            })

        except Exception as e:
            continue

    return discussions


def fetch_github_trending_ai(limit=10) -> List[Dict]:
    """
    è·å– GitHub çƒ­é—¨ AI é¡¹ç›®

    é€šè¿‡æœç´¢ API æˆ–ä½¿ç”¨ç¬¬ä¸‰æ–¹æœåŠ¡
    """
    print("ğŸ™ æ­£åœ¨ä» GitHub è·å–çƒ­é—¨AIé¡¹ç›®...")

    # ä½¿ç”¨ GitHub API æœç´¢ï¼ˆéœ€è¦tokenï¼‰æˆ–ä½¿ç”¨ç¬¬ä¸‰æ–¹æœåŠ¡
    # è¿™é‡Œç®€åŒ–ä¸ºæ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…å¯ä»¥é›†æˆ GitHub API

    trending_projects = [
        {
            "name": "autogen",
            "author": "microsoft",
            "description": "Enable next-gen LLM applications. Converse, collaborate, and code with agents.",
            "stars": 28000,
            "url": "https://github.com/microsoft/autogen",
            "tags": ["å¤šæ™ºèƒ½ä½“", "LLM", "æ¡†æ¶"],
        },
        {
            "name": "langchain",
            "author": "langchain-ai",
            "description": "Building applications with LLMs through composability",
            "stars": 76000,
            "url": "https://github.com/langchain-ai/langchain",
            "tags": ["LLM", "RAG", "æ¡†æ¶"],
        },
        {
            "name": "vllm",
            "author": "vllm-project",
            "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
            "stars": 21000,
            "url": "https://github.com/vllm-project/vllm",
            "tags": ["æ¨ç†åŠ é€Ÿ", "LLM", "éƒ¨ç½²"],
        },
        {
            "name": "open-interpreter",
            "author": "OpenInterpreter",
            "description": "Open Interpreter lets LLMs run code (Python, JS, Shell, etc.) on your computer",
            "stars": 46000,
            "url": "https://github.com/OpenInterpreter/open-interpreter",
            "tags": ["ä»£ç æ‰§è¡Œ", "AIåŠ©æ‰‹", "å·¥å…·"],
        },
        {
            "name": "ComfyUI",
            "author": "comfyanonymous",
            "description": "A powerful and modular stable diffusion GUI",
            "stars": 39000,
            "url": "https://github.com/comfyanonymous/ComfyUI",
            "tags": ["å›¾åƒç”Ÿæˆ", "GUI", "æ‰©æ•£æ¨¡å‹"],
        },
    ]

    return trending_projects[:limit]


def generate_weekly_report() -> Dict:
    """
    ç”Ÿæˆå‘¨æŠ¥æ•°æ®
    """
    print("\nğŸš€ å¼€å§‹é‡‡é›†AIå‰æ²¿ä¿¡æ¯...\n")

    data = {
        "report_date": datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥"),
        "week_number": datetime.now().isocalendar()[1],
        "papers": fetch_arxiv_papers(days_back=7, limit=15),
        "hn_discussions": fetch_hacker_news_ai(limit=8),
        "github_trending": fetch_github_trending_ai(limit=6),
    }

    print(f"\nâœ… é‡‡é›†å®Œæˆï¼")
    print(f"   ğŸ“„ è®ºæ–‡: {len(data['papers'])} ç¯‡")
    print(f"   ğŸ’¬ HNè®¨è®º: {len(data['hn_discussions'])} æ¡")
    print(f"   ğŸ™ GitHubé¡¹ç›®: {len(data['github_trending'])} ä¸ª")

    return data


def save_data(data: Dict, filename: str = None):
    """
    ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
    """
    if filename is None:
        filename = f"ai_insights_{datetime.now().strftime('%Y%m%d')}.json"

    filepath = os.path.join(OUTPUT_DIR, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
    return filepath


if __name__ == "__main__":
    # é‡‡é›†æ•°æ®
    data = generate_weekly_report()

    # ä¿å­˜æ•°æ®
    save_data(data)

    print("\nğŸ‰ å®Œæˆï¼ç°åœ¨å¯ä»¥ç”Ÿæˆ HTML æŠ¥å‘Šäº†")
    print("   è¿è¡Œ: python generate_html.py")
