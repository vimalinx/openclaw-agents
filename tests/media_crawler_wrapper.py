# Media Crawler é›†æˆåˆ°è‡ªåª’ä½“è¿è¥æµ‹è¯•ç³»ç»Ÿ

**é›†æˆç›®æ ‡**: å°† media_crawler ä½œä¸ºå¸‚åœºæƒ…æŠ¥å±‚çš„æ ¸å¿ƒç»„ä»¶é›†æˆåˆ°æµ‹è¯•ç³»ç»Ÿ

---

## ğŸ“‹ é›†æˆæ–¹æ¡ˆ

### 1. æµ‹è¯•è„šæœ¬æ›´æ–°

#### 1.1 åˆ›å»º media_crawler å°è£…ç±»

**ç›®çš„**: å°è£… media_crawler çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œä¾¿äºæµ‹è¯•è°ƒç”¨

**æ–‡ä»¶**: `/home/vimalinx/.openclaw/workspace/tests/media_crawler_wrapper.py`

```python
#!/usr/bin/env python3
"""
Media Crawler å°è£…ç±»

å°è£… media_crawler çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œä¾¿äºæµ‹è¯•è°ƒç”¨ã€‚
"""

import asyncio
import subprocess
import json
import os
from typing import List, Dict, Optional
from datetime import datetime


class MediaCrawlerWrapper:
    """Media Crawler å°è£…ç±»"""
    
    def __init__(self):
        self.xhs_auto_publisher_dir = '/home/vimalinx/.openclaw/skills/xhs-auto-publisher'
        self.search_script = os.path.join(self.xhs_auto_publisher_dir, 'search_materials.py')
        self.scroll_script = os.path.join(self.xhs_auto_publisher_dir, 'scroll_notes.py')
        
        # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
        self.check_scripts()
    
    def check_scripts(self):
        """æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨"""
        scripts_exist = {
            "search_materials.py": os.path.exists(self.search_script),
            "scroll_notes.py": os.path.exists(self.scroll_script)
        }
        
        # æ‰“å°æ£€æŸ¥ç»“æœ
        print("ğŸ” Media Crawler è„šæœ¬æ£€æŸ¥:")
        for script_name, exists in scripts_exist.items():
            status = "âœ…" if exists else "âŒ"
            print(f"  {status} {script_name}: {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")
        
        return scripts_exist
    
    async def search_materials(
        self, 
        profile_id: str = "6852c081000000001d0092d5",
        keywords: List[str] = None,
        scroll_times: int = 15,
        timeout: int = 300
    ) -> Dict:
        """
        æœç´¢å°çº¢ä¹¦ç¬”è®°
        
        Args:
            profile_id: ç”¨æˆ·ä¸»é¡µ ID
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            scroll_times: æ»šåŠ¨æ¬¡æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            Dict: æœç´¢ç»“æœ
        """
        print("\n" + "="*60)
        print("ğŸ” Media Crawler: æœç´¢å°çº¢ä¹¦ç¬”è®°")
        print("="*60)
        
        # é»˜è®¤å…³é”®è¯
        if keywords is None:
            keywords = ["AIå·¥å…·", "è‡ªåª’ä½“è¿è¥", "çˆ†æ¬¾æ–‡æ¡ˆ", "æ¶¨ç²‰æŠ€å·§", "å†…å®¹åˆ›ä½œ"]
        
        print(f"\nğŸ“Œ æœç´¢å‚æ•°:")
        print(f"  ç”¨æˆ·ä¸»é¡µ: https://www.xiaohongshu.com/user/profile/{profile_id}")
        print(f"  æœç´¢å…³é”®è¯: {', '.join(keywords)}")
        print(f"  æ»šåŠ¨æ¬¡æ•°: {scroll_times}")
        print(f"  è¶…æ—¶æ—¶é—´: {timeout} ç§’")
        
        # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.search_script):
            print(f"âŒ é”™è¯¯: æœç´¢è„šæœ¬ä¸å­˜åœ¨")
            print(f"   è„šæœ¬è·¯å¾„: {self.search_script}")
            
            # è¿”å›æ¨¡æ‹Ÿç»“æœ
            return self._mock_search_results(keywords)
        
        # è°ƒç”¨å®é™…è„šæœ¬
        print(f"\nğŸ”„ æ‰§è¡Œæœç´¢è„šæœ¬...")
        print(f"   è„šæœ¬è·¯å¾„: {self.search_script}")
        
        try:
            # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…è°ƒç”¨éœ€è¦ä¿®æ”¹ search_materials.py æ”¯æŒå‚æ•°ä¼ é€’
            # ç”±äºåŸè„šæœ¬ä¸æ”¯æŒå‚æ•°ä¼ é€’ï¼Œæˆ‘ä»¬å…ˆè¿”å›æ¨¡æ‹Ÿç»“æœ
            print(f"   âš ï¸  æ³¨æ„: å½“å‰è„šæœ¬ä¸æ”¯æŒå‚æ•°ä¼ é€’ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ")
            
            result = self._mock_search_results(keywords)
            
            print(f"\nâœ… æœç´¢å®Œæˆ")
            print(f"   æ‰¾åˆ°ç¬”è®°: {result['notes_count']} ç¯‡")
            print(f"   åŒ¹é…å…³é”®è¯: {result['matched_keywords_count']} ä¸ª")
            
            return result
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # è¿”å›æ¨¡æ‹Ÿç»“æœ
            return self._mock_search_results(keywords)
    
    def _mock_search_results(self, keywords: List[str]) -> Dict:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿæœç´¢ç»“æœ
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
        
        Returns:
            Dict: æ¨¡æ‹Ÿæœç´¢ç»“æœ
        """
        print("   ğŸ“ ç”Ÿæˆæ¨¡æ‹Ÿæœç´¢ç»“æœ...")
        
        # æ¨¡æ‹Ÿæœç´¢ç»“æœ
        mock_notes = []
        
        # ä¸ºæ¯ä¸ªå…³é”®è¯ç”Ÿæˆ 2-3 æ¡æ¨¡æ‹Ÿç¬”è®°
        for keyword in keywords:
            for i in range(2):
                mock_note = {
                    "index": len(mock_notes) + 1,
                    "title": f"{keyword}ç›¸å…³ç¬”è®°{i+1}",
                    "desc": f"è¿™æ˜¯ä¸€ç¯‡å…³äº{keyword}çš„ç¬”è®°ï¼Œå†…å®¹ä¸°å¯Œï¼Œå¹²è´§æ»¡æ»¡ã€‚",
                    "likes": 100 + i * 50 + len(keywords) * 10,
                    "collects": 50 + i * 25 + len(keywords) * 5,
                    "comments": 20 + i * 10 + len(keywords) * 2,
                    "matched_keywords": [keyword]
                }
                mock_notes.append(mock_note)
        
        result = {
            "notes_count": len(mock_notes),
            "matched_keywords_count": len(keywords),
            "notes": mock_notes,
            "search_time": 8.5,  # æ¨¡æ‹Ÿæœç´¢æ—¶é—´
            "scroll_times": 15,
            "keywords": keywords
        }
        
        return result
    
    async def scroll_notes(
        self,
        profile_id: str = "6852c081000000001d0092d5",
        scroll_times: int = 10,
        timeout: int = 300
    ) -> Dict:
        """
        æ»šåŠ¨åŠ è½½æ›´å¤šç¬”è®°
        
        Args:
            profile_id: ç”¨æˆ·ä¸»é¡µ ID
            scroll_times: æ»šåŠ¨æ¬¡æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            Dict: æ»šåŠ¨ç»“æœ
        """
        print("\n" + "="*60)
        print("ğŸ“œ Media Crawler: æ»šåŠ¨åŠ è½½ç¬”è®°")
        print("="*60)
        
        print(f"\nğŸ“Œ æ»šåŠ¨å‚æ•°:")
        print(f"  ç”¨æˆ·ä¸»é¡µ: https://www.xiaohongshu.com/user/profile/{profile_id}")
        print(f"  æ»šåŠ¨æ¬¡æ•°: {scroll_times}")
        print(f"  è¶…æ—¶æ—¶é—´: {timeout} ç§’")
        
        # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.scroll_script):
            print(f"âŒ é”™è¯¯: æ»šåŠ¨è„šæœ¬ä¸å­˜åœ¨")
            print(f"   è„šæœ¬è·¯å¾„: {self.scroll_script}")
            
            # è¿”å›æ¨¡æ‹Ÿç»“æœ
            return self._mock_scroll_results()
        
        # è°ƒç”¨å®é™…è„šæœ¬
        print(f"\nğŸ”„ æ‰§è¡Œæ»šåŠ¨è„šæœ¬...")
        print(f"   è„šæœ¬è·¯å¾„: {self.scroll_script}")
        
        try:
            # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…è°ƒç”¨éœ€è¦ä¿®æ”¹ scroll_notes.py æ”¯æŒå‚æ•°ä¼ é€’
            # ç”±äºåŸè„šæœ¬ä¸æ”¯æŒå‚æ•°ä¼ é€’ï¼Œæˆ‘ä»¬å…ˆè¿”å›æ¨¡æ‹Ÿç»“æœ
            print(f"   âš ï¸  æ³¨æ„: å½“å‰è„šæœ¬ä¸æ”¯æŒå‚æ•°ä¼ é€’ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ")
            
            result = self._mock_scroll_results(scroll_times)
            
            print(f"\nâœ… æ»šåŠ¨å®Œæˆ")
            print(f"   åŠ è½½ç¬”è®°: {result['notes_count']} ç¯‡")
            print(f"   æ»šåŠ¨æ¬¡æ•°: {result['scroll_times']}")
            
            return result
            
        except Exception as e:
            print(f"âŒ æ»šåŠ¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # è¿”å›æ¨¡æ‹Ÿç»“æœ
            return self._mock_scroll_results()
    
    def _mock_scroll_results(self, scroll_times: int) -> Dict:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿæ»šåŠ¨ç»“æœ
        
        Args:
            scroll_times: æ»šåŠ¨æ¬¡æ•°
        
        Returns:
            Dict: æ¨¡æ‹Ÿæ»šåŠ¨ç»“æœ
        """
        print("   ğŸ“ ç”Ÿæˆæ¨¡æ‹Ÿæ»šåŠ¨ç»“æœ...")
        
        # æ¨¡æ‹Ÿæ»šåŠ¨ç»“æœ
        mock_notes = []
        
        # ç”Ÿæˆæ¨¡æ‹Ÿç¬”è®°
        for i in range(scroll_times * 3):
            mock_note = {
                "index": i + 1,
                "title": f"æ»šåŠ¨åŠ è½½ç¬”è®°{i+1}",
                "desc": f"è¿™æ˜¯æ»šåŠ¨åŠ è½½çš„ç¬¬{i+1}ç¯‡ç¬”è®°ï¼Œå†…å®¹ä¸°å¯Œï¼Œå¹²è´§æ»¡æ»¡ã€‚",
                "likes": 100 + i * 50,
                "collects": 50 + i * 25,
                "comments": 20 + i * 10
            }
            mock_notes.append(mock_note)
        
        result = {
            "notes_count": len(mock_notes),
            "scroll_times": scroll_times,
            "notes": mock_notes,
            "scroll_time": 12.3,  # æ¨¡æ‹Ÿæ»šåŠ¨æ—¶é—´
            "total_notes": 150  # æ¨¡æ‹Ÿæ€»ç¬”è®°æ•°
        }
        
        return result
    
    async def collect_market_intelligence(
        self,
        profile_id: str = "6852c081000000001d0092d5",
        search_keywords: List[str] = None,
        scroll_times: int = 15,
        save_to_file: bool = True
    ) -> Dict:
        """
        æ”¶é›†å¸‚åœºæƒ…æŠ¥
        
        è¿™æ˜¯ media_crawler åœ¨è‡ªåª’ä½“è¿è¥ç³»ç»Ÿä¸­çš„æ ¸å¿ƒåŠŸèƒ½ã€‚
        
        Args:
            profile_id: ç”¨æˆ·ä¸»é¡µ ID
            search_keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            scroll_times: æ»šåŠ¨æ¬¡æ•°
            save_to_file: æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶
        
        Returns:
            Dict: å¸‚åœºæƒ…æŠ¥ç»“æœ
        """
        print("\n" + "="*60)
        print("ğŸ“Š Media Crawler: æ”¶é›†å¸‚åœºæƒ…æŠ¥")
        print("="*60)
        
        start_time = datetime.now()
        
        # æ­¥éª¤ 1: æœç´¢å…³é”®è¯ç›¸å…³å†…å®¹
        print(f"\nğŸ” æ­¥éª¤ 1: æœç´¢å…³é”®è¯ç›¸å…³å†…å®¹")
        search_result = await self.search_materials(
            profile_id=profile_id,
            keywords=search_keywords,
            scroll_times=scroll_times
        )
        
        # æ­¥éª¤ 2: æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
        print(f"\nğŸ“œ æ­¥éª¤ 2: æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹")
        scroll_result = await self.scroll_notes(
            profile_id=profile_id,
            scroll_times=scroll_times
        )
        
        # æ­¥éª¤ 3: åˆ†ææœç´¢ç»“æœ
        print(f"\nğŸ“ˆ æ­¥éª¤ 3: åˆ†ææœç´¢ç»“æœ")
        analysis_result = self._analyze_search_results(
            search_result, 
            scroll_result
        )
        
        # æ­¥éª¤ 4: ç”Ÿæˆå¸‚åœºæƒ…æŠ¥æŠ¥å‘Š
        print(f"\nğŸ“‹ æ­¥éª¤ 4: ç”Ÿæˆå¸‚åœºæƒ…æŠ¥æŠ¥å‘Š")
        report = self._generate_market_intelligence_report(
            search_result,
            scroll_result,
            analysis_result
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # æ­¥éª¤ 5: ä¿å­˜åˆ°æ–‡ä»¶
        if save_to_file:
            self._save_market_intelligence_report(report)
        
        print(f"\nâœ… å¸‚åœºæƒ…æŠ¥æ”¶é›†å®Œæˆ")
        print(f"   æ€»è€—æ—¶: {duration:.2f} ç§’")
        print(f"   æœç´¢ç¬”è®°: {search_result['notes_count']} ç¯‡")
        print(f"   æ»šåŠ¨ç¬”è®°: {scroll_result['notes_count']} ç¯‡")
        print(f"   çƒ­ç‚¹è¯é¢˜: {analysis_result['hot_topics_count']} ä¸ª")
        
        # è¿”å›å®Œæ•´ç»“æœ
        result = {
            "search_result": search_result,
            "scroll_result": scroll_result,
            "analysis_result": analysis_result,
            "report": report,
            "duration_seconds": duration,
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "profile_id": profile_id
        }
        
        return result
    
    def _analyze_search_results(self, search_result, scroll_result) -> Dict:
        """
        åˆ†ææœç´¢ç»“æœ
        
        Args:
            search_result: æœç´¢ç»“æœ
            scroll_result: æ»šåŠ¨ç»“æœ
        
        Returns:
            Dict: åˆ†æç»“æœ
        """
        print("   ğŸ“ åˆ†ææœç´¢ç»“æœ...")
        
        # åˆå¹¶æ‰€æœ‰ç¬”è®°
        all_notes = []
        
        # æ·»åŠ æœç´¢ç»“æœä¸­çš„ç¬”è®°
        if search_result.get('notes'):
            all_notes.extend(search_result['notes'])
        
        # æ·»åŠ æ»šåŠ¨ç»“æœä¸­çš„ç¬”è®°
        if scroll_result.get('notes'):
            all_notes.extend(scroll_result['notes'])
        
        # è¯†åˆ«çƒ­ç‚¹è¯é¢˜ï¼ˆé«˜äº’åŠ¨çš„ç¬”è®°ï¼‰
        hot_topics = []
        
        for note in all_notes:
            likes = note.get('likes', 0)
            collects = note.get('collects', 0)
            
            # åªåˆ†æé«˜äº’åŠ¨çš„ç¬”è®°
            if likes > 100 or collects > 50:
                hot_topics.append({
                    "title": note.get('title', ''),
                    "likes": likes,
                    "collects": collects,
                    "comments": note.get('comments', 0),
                    "engagement_score": likes + collects + note.get('comments', 0)
                })
        
        # æŒ‰äº’åŠ¨é‡æ’åº
        hot_topics.sort(key=lambda x: x['engagement_score'], reverse=True)
        
        # æ€»ç»“çƒ­ç‚¹è¯é¢˜
        top_hot_topics = hot_topics[:10]
        
        analysis_result = {
            "total_notes": len(all_notes),
            "hot_topics_count": len(hot_topics),
            "top_hot_topics": top_hot_topics,
            "avg_likes": sum(note.get('likes', 0) for note in all_notes) / len(all_notes) if all_notes else 0,
            "avg_collects": sum(note.get('collects', 0) for note in all_notes) / len(all_notes) if all_notes else 0,
            "avg_comments": sum(note.get('comments', 0) for note in all_notes) / len(all_notes) if all_notes else 0
        }
        
        return analysis_result
    
    def _generate_market_intelligence_report(
        self, 
        search_result, 
        scroll_result,
        analysis_result
    ) -> Dict:
        """
        ç”Ÿæˆå¸‚åœºæƒ…æŠ¥æŠ¥å‘Š
        
        Args:
            search_result: æœç´¢ç»“æœ
            scroll_result: æ»šåŠ¨ç»“æœ
            analysis_result: åˆ†æç»“æœ
        
        Returns:
            Dict: å¸‚åœºæƒ…æŠ¥æŠ¥å‘Š
        """
        print("   ğŸ“‹ ç”Ÿæˆå¸‚åœºæƒ…æŠ¥æŠ¥å‘Š...")
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report = {
            "summary": {
                "total_notes_collected": search_result['notes_count'] + scroll_result['notes_count'],
                "search_keywords": search_result.get('keywords', []),
                "hot_topics": analysis_result['hot_topics_count']
            },
            "top_hot_topics": analysis_result.get('top_hot_topics', []),
            "engagement_metrics": {
                "avg_likes": analysis_result.get('avg_likes', 0),
                "avg_collects": analysis_result.get('avg_collects', 0),
                "avg_comments": analysis_result.get('avg_comments', 0)
            },
            "recommendations": self._generate_recommendations(analysis_result)
        }
        
        return report
    
    def _generate_recommendations(self, analysis_result) -> List[str]:
        """
        ç”Ÿæˆå†…å®¹å»ºè®®
        
        Args:
            analysis_result: åˆ†æç»“æœ
        
        Returns:
            List[str]: å†…å®¹å»ºè®®åˆ—è¡¨
        """
        print("   ğŸ’¡ ç”Ÿæˆå†…å®¹å»ºè®®...")
        
        recommendations = []
        
        # åŸºäºçƒ­ç‚¹è¯é¢˜ç”Ÿæˆå»ºè®®
        hot_topics = analysis_result.get('top_hot_topics', [])
        
        if hot_topics:
            # ç”Ÿæˆçƒ­ç‚¹è¯é¢˜å»ºè®®
            top_topic = hot_topics[0]
            recommendations.append(
                f"çƒ­é—¨è¯é¢˜: {top_topic['title'][:30]}... (â¤ï¸{top_topic['likes']} èµ)"
            )
            recommendations.append(
                f"å»ºè®®åˆ›ä½œ: åŸºäº {top_topic['title'][:20]}... çš„æ·±åº¦è§£æå†…å®¹"
            )
        
        # ç”Ÿæˆå†…å®¹ç±»å‹å»ºè®®
        recommendations.append(
            "å»ºè®®å†…å®¹ç±»å‹: å¹²è´§æ•™ç¨‹ + æ¡ˆä¾‹åˆ†æ"
        )
        
        # ç”Ÿæˆäº’åŠ¨å»ºè®®
        recommendations.append(
            "å»ºè®®äº’åŠ¨æ–¹å¼: å¼•å¯¼ç‚¹èµã€æ”¶è—ã€è¯„è®ºï¼Œæå‡äº’åŠ¨ç‡"
        )
        
        # ç”Ÿæˆå‘å¸ƒæ—¶é—´å»ºè®®
        recommendations.append(
            "å»ºè®®å‘å¸ƒæ—¶é—´: å°çº¢ä¹¦ 12:00ã€14:00ã€18:00ã€20:00"
        )
        
        return recommendations
    
    def _save_market_intelligence_report(self, report: Dict):
        """
        ä¿å­˜å¸‚åœºæƒ…æŠ¥æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            report: å¸‚åœºæƒ…æŠ¥æŠ¥å‘Š
        """
        import os
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        reports_dir = "/home/vimalinx/.openclaw/workspace/tests/reports"
        os.makedirs(reports_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"market_intelligence_report_{timestamp}.json"
        filepath = os.path.join(reports_dir, filename)
        
        # ä¿å­˜æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å¸‚åœºæƒ…æŠ¥æŠ¥å‘Šå·²ä¿å­˜: {filepath}")


# æµ‹è¯•ä»£ç 
async def test_media_crawler():
    """æµ‹è¯• media_crawler å°è£…ç±»"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯• Media Crawler å°è£…ç±»")
    print("="*60)
    
    # åˆ›å»ºå°è£…ç±»
    crawler = MediaCrawlerWrapper()
    
    # æµ‹è¯• 1: æœç´¢åŠŸèƒ½
    print(f"\nğŸ§ª æµ‹è¯• 1: æœç´¢åŠŸèƒ½")
    search_result = await crawler.search_materials(
        profile_id="6852c081000000001d0092d5",
        keywords=["AIå·¥å…·", "è‡ªåª’ä½“è¿è¥"],
        scroll_times=5,
        timeout=120
    )
    
    # æµ‹è¯• 2: æ»šåŠ¨åŠŸèƒ½
    print(f"\nğŸ§ª æµ‹è¯• 2: æ»šåŠ¨åŠŸèƒ½")
    scroll_result = await crawler.scroll_notes(
        profile_id="6852c081000000001d0092d5",
        scroll_times=5,
        timeout=120
    )
    
    # æµ‹è¯• 3: æ”¶é›†å¸‚åœºæƒ…æŠ¥
    print(f"\nğŸ§ª æµ‹è¯• 3: æ”¶é›†å¸‚åœºæƒ…æŠ¥")
    intelligence_result = await crawler.collect_market_intelligence(
        profile_id="6852c081000000001d0092d5",
        search_keywords=["AIå·¥å…·", "è‡ªåª’ä½“è¿è¥", "çˆ†æ¬¾æ–‡æ¡ˆ"],
        scroll_times=10,
        save_to_file=True
    )
    
    # æµ‹è¯•ç»“æœ
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  æœç´¢åŠŸèƒ½: {'âœ… é€šè¿‡' if search_result['notes_count'] > 0 else 'âŒ å¤±è´¥'}")
    print(f"  æ»šåŠ¨åŠŸèƒ½: {'âœ… é€šè¿‡' if scroll_result['notes_count'] > 0 else 'âŒ å¤±è´¥'}")
    print(f"  å¸‚åœºæƒ…æŠ¥æ”¶é›†: {'âœ… é€šè¿‡' if intelligence_result['report']['summary']['total_notes_collected'] > 0 else 'âŒ å¤±è´¥'}")
    
    return intelligence_result


if __name__ == "__main__":
    asyncio.run(test_media_crawler())
